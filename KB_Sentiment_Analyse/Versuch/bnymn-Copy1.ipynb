{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a22cd9684b1ccf2bf483b55f746881ebcbbcbe4c"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9b15f7daa8f9c5cb04c1ff8974a9c0cefb672f6f"
   },
   "source": [
    "Sentiment analysis is part of the Natural Language Processing (NLP) techniques that consists in extracting emotions related to some raw texts. This is usually used on social media posts and customer reviews in order to automatically understand if some users are positive or negative and why. The goal of this study is to show how sentiment analysis can be performed using python. Here are some of the main libraries we will use:\n",
    "\n",
    "- NLTK: the most famous python module for NLP techniques\n",
    "- Gensim: a topic-modelling and vector space modelling toolkit\n",
    "- Scikit-learn: the most used python machine learning library\n",
    "\n",
    "We will use here some hotel reviews data. Each observation consists in one customer review for one hotel. Each customer review is composed of a textual feedback of the customer's experience at the hotel and an overall rating. The data can be found here:\n",
    "https://www.kaggle.com/jiashenliu/515k-hotel-reviews-data-in-europe\n",
    "\n",
    "For each textual review, we want to predict if it corresponds to a good review (the customer is happy) or to a bad one (the customer is not satisfied). The reviews overall ratings can range from 2.5/10 to 10/10. In order to simplify the problem we will split those into two categories:\n",
    "- bad reviews have overall ratings < 5\n",
    "- good reviews have overall ratings >= 5\n",
    "\n",
    "The challenge here is to be able to predict this information using only the raw textual data from the review.\n",
    "Let's get it started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c52144478e304f76ce5310ab30a72a08e0c35dd"
   },
   "source": [
    "We first start by loading the raw data. Each textual reviews is splitted into a positive part and a negative part. We group them together in order to start with only raw text data and no other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-c85b41194eab>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-c85b41194eab>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    -m pip install wordcloud\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "-m pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read data\n",
    "reviews_df = pd.read_csv(\"kbClean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df['review'] = reviews_df['Erfahrungsbericht']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>NameKlinik</th>\n",
       "      <th>Erfahrungsbericht</th>\n",
       "      <th>Gesamt</th>\n",
       "      <th>QualBeratung</th>\n",
       "      <th>MedBehandlung</th>\n",
       "      <th>VerwalAblaeufe</th>\n",
       "      <th>Zufriedenheit</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HEH Herzogin Elisabeth Hospital</td>\n",
       "      <td>Sehr höfliche und hilfsbereite Mitarbeiter.  S...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>Sehr höfliche und hilfsbereite Mitarbeiter.  S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>HEH Herzogin Elisabeth Hospital</td>\n",
       "      <td>Kann ich nur weiterempfehlen!! Sehr gutes Pfle...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>pos</td>\n",
       "      <td>Kann ich nur weiterempfehlen!! Sehr gutes Pfle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>HEH Herzogin Elisabeth Hospital</td>\n",
       "      <td>Bestens durchorganisiert.gutes Personal geht i...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>Bestens durchorganisiert.gutes Personal geht i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>HEH Herzogin Elisabeth Hospital</td>\n",
       "      <td>Bin nach einem Autounfall im HEH gewesen und h...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>Bin nach einem Autounfall im HEH gewesen und h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>HEH Herzogin Elisabeth Hospital</td>\n",
       "      <td>Wurde bereits im Sept. 2020 am Knie operiert. ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>Wurde bereits im Sept. 2020 am Knie operiert. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       NameKlinik  \\\n",
       "0           0  HEH Herzogin Elisabeth Hospital   \n",
       "1           1  HEH Herzogin Elisabeth Hospital   \n",
       "2           2  HEH Herzogin Elisabeth Hospital   \n",
       "3           3  HEH Herzogin Elisabeth Hospital   \n",
       "4           4  HEH Herzogin Elisabeth Hospital   \n",
       "\n",
       "                                   Erfahrungsbericht  Gesamt  QualBeratung  \\\n",
       "0  Sehr höfliche und hilfsbereite Mitarbeiter.  S...     5.0           5.0   \n",
       "1  Kann ich nur weiterempfehlen!! Sehr gutes Pfle...     5.0           3.7   \n",
       "2  Bestens durchorganisiert.gutes Personal geht i...     5.0           5.0   \n",
       "3  Bin nach einem Autounfall im HEH gewesen und h...     5.0           5.0   \n",
       "4  Wurde bereits im Sept. 2020 am Knie operiert. ...     5.0           5.0   \n",
       "\n",
       "   MedBehandlung  VerwalAblaeufe Zufriedenheit  \\\n",
       "0            5.0             5.0           pos   \n",
       "1            5.0             3.7           pos   \n",
       "2            5.0             5.0           pos   \n",
       "3            5.0             5.0           pos   \n",
       "4            5.0             5.0           pos   \n",
       "\n",
       "                                              review  \n",
       "0  Sehr höfliche und hilfsbereite Mitarbeiter.  S...  \n",
       "1  Kann ich nur weiterempfehlen!! Sehr gutes Pfle...  \n",
       "2  Bestens durchorganisiert.gutes Personal geht i...  \n",
       "3  Bin nach einem Autounfall im HEH gewesen und h...  \n",
       "4  Wurde bereits im Sept. 2020 am Knie operiert. ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yigit/opt/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "cb56534fa07076079598d82a2e69848802148d70"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>is_bad_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sehr höfliche und hilfsbereite Mitarbeiter.  S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kann ich nur weiterempfehlen!! Sehr gutes Pfle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bestens durchorganisiert.gutes Personal geht i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bin nach einem Autounfall im HEH gewesen und h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wurde bereits im Sept. 2020 am Knie operiert. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  is_bad_review\n",
       "0  Sehr höfliche und hilfsbereite Mitarbeiter.  S...              0\n",
       "1  Kann ich nur weiterempfehlen!! Sehr gutes Pfle...              0\n",
       "2  Bestens durchorganisiert.gutes Personal geht i...              0\n",
       "3  Bin nach einem Autounfall im HEH gewesen und h...              0\n",
       "4  Wurde bereits im Sept. 2020 am Knie operiert. ...              0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# append the positive and negative text reviews\n",
    "#reviews_df[\"review\"] = reviews_df[\"Negative_Review\"] + reviews_df[\"Positive_Review\"]\n",
    "# create the label\n",
    "reviews_df[\"is_bad_review\"] = reviews_df[\"Zufriedenheit\"].apply(lambda x: 1 if x == 'neg' else 0)\n",
    "# select only relevant columns\n",
    "reviews_df = reviews_df[[\"review\", \"is_bad_review\"]]\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "137871905d305dae06cbc7c9f263b3ac3a6b85b7"
   },
   "source": [
    "# Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "a4f04ca1ef2a8b8b98bdbba2a3563b776a583964"
   },
   "outputs": [],
   "source": [
    "reviews_df = reviews_df.sample(frac = 0.1, replace = False, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f57e5b9e22b0ac6d9a1fb98224da2d2417e991f4"
   },
   "source": [
    "Reviews data is sampled in order to speed up computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f2061b96461095869fe3cbd19ae10ca9c651482"
   },
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "005e87154e11f530b58c56cac9f555c82493b906"
   },
   "outputs": [],
   "source": [
    "# remove 'No Negative' or 'No Positive' from text\n",
    "reviews_df[\"review\"] = reviews_df[\"review\"].apply(lambda x: x.replace(\"No Negative\", \"\").replace(\"No Positive\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "256f6d920bcb8b50725cb93488a9e7be4c772349"
   },
   "source": [
    "If the user doesn't leave any negative feedback comment, this will appear as \"No Negative\" in our data. This is the same for the positive comments with the default value \"No Positive\". We have to remove those parts from our texts.\n",
    "\n",
    "The next step consists in cleaning the text data with various operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "2d51a84a81d55c527dbd56b42201f5df3d444ce5"
   },
   "outputs": [],
   "source": [
    "# return the wordnet object value corresponding to the POS tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_text(text):\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    # tokenize text and remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove words that contain numbers\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    text = [x for x in text if x not in stop]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # pos tag text\n",
    "    pos_tags = pos_tag(text)\n",
    "    # lemmatize text\n",
    "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "    return(text)\n",
    "\n",
    "# clean text data\n",
    "reviews_df[\"review_clean\"] = reviews_df[\"review\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab1d87cea172f979af13ba58cafed1302267b183"
   },
   "source": [
    "To clean textual data, we call our custom 'clean_text' function that performs several transformations:\n",
    "- lower the text\n",
    "- tokenize the text (split the text into words) and remove the punctuation\n",
    "- remove useless words that contain numbers\n",
    "- remove useless stop words like 'the', 'a' ,'this' etc.\n",
    "- Part-Of-Speech (POS) tagging: assign a tag to every word to define if it corresponds to a noun, a verb etc. using the WordNet lexical database\n",
    "- lemmatize the text: transform every word into their root form (e.g. rooms -> room, slept -> sleep)\n",
    "\n",
    "Now that we have cleaned our data, we can do some feature engineering for our modelization part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79ed7c7647f7868d74bb5b3267a77914539dd61d"
   },
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "edbc356b3b7289c9aaafeafb41c11eb7c5a601f7"
   },
   "outputs": [],
   "source": [
    "# add sentiment anaylsis columns\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "reviews_df[\"sentiments\"] = reviews_df[\"review\"].apply(lambda x: sid.polarity_scores(x))\n",
    "reviews_df = pd.concat([reviews_df.drop(['sentiments'], axis=1), reviews_df['sentiments'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0c9161a58d443d38ca818ef7deb128cc92359b5c"
   },
   "source": [
    "We first start by adding sentiment analysis features because we can guess that customers reviews are highly linked to how they felt about their stay at the hotel. We use Vader, which is a part of the NLTK module designed for sentiment analysis. Vader uses a lexicon of words to find which ones are positives or negatives. It also takes into accout the context of the sentences to determine the sentiment scores. For each text, Vader retuns 4 values:\n",
    "- a neutrality score\n",
    "- a positivity score\n",
    "- a negativity score\n",
    "- an overall score that summarizes the previous scores\n",
    "\n",
    "We will integrate those 4 values as features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "183c8e5ad5659fea721bed9cdf40b45757f49721"
   },
   "outputs": [],
   "source": [
    "# add number of characters column\n",
    "reviews_df[\"nb_chars\"] = reviews_df[\"review\"].apply(lambda x: len(x))\n",
    "\n",
    "# add number of words column\n",
    "reviews_df[\"nb_words\"] = reviews_df[\"review\"].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "892e8afaecce1fbfcbd61cfc5cbfea2a122c0296"
   },
   "source": [
    "Next, we add some simple metrics for every text:\n",
    "- number of characters in the text\n",
    "- number of words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "905b125a775fa13b20d869443dc439d324a124b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yigit/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# create doc2vec vector columns\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(reviews_df[\"review_clean\"].apply(lambda x: x.split(\" \")))]\n",
    "\n",
    "# train a Doc2Vec model with our text data\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "\n",
    "# transform each document into a vector data\n",
    "doc2vec_df = reviews_df[\"review_clean\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\n",
    "doc2vec_df.columns = [\"doc2vec_vector_\" + str(x) for x in doc2vec_df.columns]\n",
    "reviews_df = pd.concat([reviews_df, doc2vec_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b1c2be5745ae8ecf348212b5af359f3c9ce09c1"
   },
   "source": [
    "The next step consist in extracting vector representations for every review. The module Gensim creates a numerical vector representation of every word in the corpus by using the contexts in which they appear (Word2Vec). This is performed using shallow neural networks. What's interesting is that similar words will have similar representation vectors.\n",
    "\n",
    "Each text can also be transformed into numerical vectors using the word vectors (Doc2Vec). Same texts will also have similar representations and that is why we can use those vectors as training features.\n",
    "\n",
    "We first have to train a Doc2Vec model by feeding in our text data. By applying this model on our reviews, we can get those representation vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "2a2b0d4e04270e1249ec4436dce176573be2c147"
   },
   "outputs": [],
   "source": [
    "# add tf-idfs columns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df = 10)\n",
    "tfidf_result = tfidf.fit_transform(reviews_df[\"review_clean\"]).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())\n",
    "tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n",
    "tfidf_df.index = reviews_df.index\n",
    "reviews_df = pd.concat([reviews_df, tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90c32da70b76239209f524e4fa473dba8685b2a3"
   },
   "source": [
    "Finally we add the TF-IDF (Term Frequency - Inverse Document Frequency) values for every word and every document. \n",
    "\n",
    "But why not simply counting how many times each word appears in every document? The problem with this method is that it doesn't take into account the relative importance of words in the texts. A word that appears in almost every text would not likely bring useful information for analysis. On the contrary, rare words may have a lot more of meanings.\n",
    "\n",
    "The TF-IDF metric solves this problem:\n",
    "- TF computes the classic number of times the word appears in the text\n",
    "- IDF computes the relative importance of this word which depends on how many texts the word can be found\n",
    "\n",
    "We add TF-IDF columns for every word that appear in at least 10 different texts to filter some of them and reduce the size of the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "6682eda4d0078a4953f5111717e4750c8f98d409"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>is_bad_review</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>nb_chars</th>\n",
       "      <th>nb_words</th>\n",
       "      <th>doc2vec_vector_0</th>\n",
       "      <th>...</th>\n",
       "      <th>word_zustand</th>\n",
       "      <th>word_zuvorkommend</th>\n",
       "      <th>word_zwar</th>\n",
       "      <th>word_zwei</th>\n",
       "      <th>word_ärzte</th>\n",
       "      <th>word_ärzten</th>\n",
       "      <th>word_ärztin</th>\n",
       "      <th>word_ärztliche</th>\n",
       "      <th>word_äußerst</th>\n",
       "      <th>word_über</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Nach der ersten Hüft TEP OP vor 7 Jahren stand...</td>\n",
       "      <td>0</td>\n",
       "      <td>nach der ersten hüft tep op vor jahren stand n...</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.9329</td>\n",
       "      <td>451</td>\n",
       "      <td>76</td>\n",
       "      <td>0.616657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>Das widerwärtigste Essen, das ich je ertragen ...</td>\n",
       "      <td>1</td>\n",
       "      <td>da widerwärtigste essen da ich je ertragen mus...</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6360</td>\n",
       "      <td>125</td>\n",
       "      <td>21</td>\n",
       "      <td>0.267775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>Ich würde am 26.11.16 Operiert. Ich fühlte mic...</td>\n",
       "      <td>0</td>\n",
       "      <td>ich würde operiert ich fühlte mich nach der op...</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6360</td>\n",
       "      <td>159</td>\n",
       "      <td>28</td>\n",
       "      <td>0.173432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>Ich kann für diese Klinik nicht einmal einen S...</td>\n",
       "      <td>1</td>\n",
       "      <td>ich kann für diese klinik nicht einmal einen s...</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.7650</td>\n",
       "      <td>1000</td>\n",
       "      <td>138</td>\n",
       "      <td>1.402389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Die Klinik ist sowohl von der Patientenversorg...</td>\n",
       "      <td>0</td>\n",
       "      <td>die klinik ist sowohl von der patientenversorg...</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>169</td>\n",
       "      <td>26</td>\n",
       "      <td>0.310260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 459 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  is_bad_review  \\\n",
       "52    Nach der ersten Hüft TEP OP vor 7 Jahren stand...              0   \n",
       "679   Das widerwärtigste Essen, das ich je ertragen ...              1   \n",
       "969   Ich würde am 26.11.16 Operiert. Ich fühlte mic...              0   \n",
       "1251  Ich kann für diese Klinik nicht einmal einen S...              1   \n",
       "203   Die Klinik ist sowohl von der Patientenversorg...              0   \n",
       "\n",
       "                                           review_clean    neg    neu    pos  \\\n",
       "52    nach der ersten hüft tep op vor jahren stand n...  0.167  0.833  0.000   \n",
       "679   da widerwärtigste essen da ich je ertragen mus...  0.181  0.819  0.000   \n",
       "969   ich würde operiert ich fühlte mich nach der op...  0.144  0.856  0.000   \n",
       "1251  ich kann für diese klinik nicht einmal einen s...  0.054  0.931  0.015   \n",
       "203   die klinik ist sowohl von der patientenversorg...  0.135  0.865  0.000   \n",
       "\n",
       "      compound  nb_chars  nb_words  doc2vec_vector_0  ...  word_zustand  \\\n",
       "52     -0.9329       451        76          0.616657  ...           0.0   \n",
       "679    -0.6360       125        21          0.267775  ...           0.0   \n",
       "969    -0.6360       159        28          0.173432  ...           0.0   \n",
       "1251   -0.7650      1000       138          1.402389  ...           0.0   \n",
       "203    -0.5994       169        26          0.310260  ...           0.0   \n",
       "\n",
       "      word_zuvorkommend  word_zwar  word_zwei  word_ärzte  word_ärzten  \\\n",
       "52             0.000000        0.0        0.0         0.0          0.0   \n",
       "679            0.000000        0.0        0.0         0.0          0.0   \n",
       "969            0.341226        0.0        0.0         0.0          0.0   \n",
       "1251           0.000000        0.0        0.0         0.0          0.0   \n",
       "203            0.000000        0.0        0.0         0.0          0.0   \n",
       "\n",
       "      word_ärztin  word_ärztliche  word_äußerst  word_über  \n",
       "52            0.0             0.0           0.0   0.000000  \n",
       "679           0.0             0.0           0.0   0.000000  \n",
       "969           0.0             0.0           0.0   0.000000  \n",
       "1251          0.0             0.0           0.0   0.058849  \n",
       "203           0.0             0.0           0.0   0.000000  \n",
       "\n",
       "[5 rows x 459 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "90a369931c57755f171e8cd9e8f9ff8e12e7a04b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 459)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fff48629c55ba10d8625ced055d92cfd0621a235"
   },
   "source": [
    "# Exploratory data analysis\n",
    "\n",
    "\n",
    "In order to have a better understanding of our data, let's explore it a little:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "9b19f8973b5aabec726dfe0a4806e2d88a310b0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.536364\n",
       "1    0.463636\n",
       "Name: is_bad_review, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show is_bad_review distribution\n",
    "reviews_df[\"is_bad_review\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd96811d40bd891b0360b702bfc0b59a33a7215a"
   },
   "source": [
    "Our dataset is highly imbalanced because less than 5% of our reviews are considered as negative ones. This information will be very useful for the modelling part.\n",
    "\n",
    "Now let's print some wordclouds to have a glimpse at what kind of words apear in our reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "0d81a097747f9e9d16076cbaa211e7e66ba37a8f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a1727a129895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# wordcloud function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# wordcloud function\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color = 'white',\n",
    "        max_words = 200,\n",
    "        max_font_size = 40, \n",
    "        scale = 3,\n",
    "        random_state = 42\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize = (20, 20))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize = 20)\n",
    "        fig.subplots_adjust(top = 2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "# print wordcloud\n",
    "show_wordcloud(reviews_df[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cad7d1ca94c9ce4fff486c0f65450e41790ecd57"
   },
   "source": [
    "Most of the words are indeed related to the hotels: room, staff, breakfast, etc. Some words are more related to the customer experience with the hotel stay: perfect, loved, expensive, dislike, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d247f98a3ae20c5b1b8227171d12ba375d548132"
   },
   "outputs": [],
   "source": [
    "# highest positive sentiment reviews (with more than 5 words)\n",
    "reviews_df[reviews_df[\"nb_words\"] >= 5].sort_values(\"pos\", ascending = False)[[\"review\", \"pos\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca802efbb1bb92214e5addfc400952178970162a"
   },
   "source": [
    "The most positive reviews indeed correspond to some good feedbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e506946236e230f419c09280cbfbcdfad35ab1a4"
   },
   "outputs": [],
   "source": [
    "# lowest negative sentiment reviews (with more than 5 words)\n",
    "reviews_df[reviews_df[\"nb_words\"] >= 5].sort_values(\"neg\", ascending = False)[[\"review\", \"neg\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8064f8760dff31dab1fed7deed313d57655dee94"
   },
   "source": [
    "Some errors can be found among the most negative reviews: Vader sometimes interpret 'no' or 'nothing' as negative words whereas they are sometimes used to say that there were no problems with the hotel. Fortunately, most of the reviews are indeed bad ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "efd086062b8ded2dd18515d66048416e7b979a5c"
   },
   "outputs": [],
   "source": [
    "# plot sentiment distribution for positive and negative reviews\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "for x in [0, 1]:\n",
    "    subset = reviews_df[reviews_df['is_bad_review'] == x]\n",
    "    \n",
    "    # Draw the density plot\n",
    "    if x == 0:\n",
    "        label = \"Good reviews\"\n",
    "    else:\n",
    "        label = \"Bad reviews\"\n",
    "    sns.distplot(subset['compound'], hist = False, label = label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "09bfd81cce5feeb359cff985e71061c44a9c21de"
   },
   "source": [
    "The above graph shows the distribution of the reviews sentiments among good reviews and bad ones. We can see that good reviews are for most of them considered as very positive by Vader. On the contrary, bad reviews tend to have lower compound sentiment scores.\n",
    "\n",
    "This shows us that previously computed sentiment features will be very important in our modelling part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6792e4ff851b10ce6e0dc46bb8b81e066a6026e7"
   },
   "source": [
    "# Modelling reviewer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5176528611b2a898af937dedfdb89661d9fd1870"
   },
   "outputs": [],
   "source": [
    "# feature selection\n",
    "label = \"is_bad_review\"\n",
    "ignore_cols = [label, \"review\", \"review_clean\"]\n",
    "features = [c for c in reviews_df.columns if c not in ignore_cols]\n",
    "\n",
    "# split the data into train and test\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews_df[features], reviews_df[label], test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9269d829bf108611ce8ed17517309a72e2e9e727"
   },
   "source": [
    "We first choose which features we want to use to train our model. Then we split our data into two parts:\n",
    "- one to train our model\n",
    "- one to assess its performances\n",
    "\n",
    "We will next use a Random Forest (RF) classifier for our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "59086a25cfed314ea49d54d4ee58df4e194a0342"
   },
   "outputs": [],
   "source": [
    "# train a random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# show feature importance\n",
    "feature_importances_df = pd.DataFrame({\"feature\": features, \"importance\": rf.feature_importances_}).sort_values(\"importance\", ascending = False)\n",
    "feature_importances_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d492d226a93da9843ad25c36ceb89645353f1acd"
   },
   "source": [
    "The most important features are indeed the ones that come from the previous sentiment analysis. The vector representations of the texts also have a lot of importance in our training. Some words appear to have a fairly good importance as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9900998d64c493c2c162b1b2bd7b5b3e3b72db90"
   },
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = [x[1] for x in rf.predict_proba(X_test)]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label = 1)\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(1, figsize = (15, 10))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65e4d155a09ff2750eed793b890bd99c35960da8"
   },
   "source": [
    "The ROC (Receiver Operating Characteristic) curve is usually a good graph to summarize the quality of our classifier. The higher the curve is above the diagonal baseline, the better the predictions. Although the AUC ROC (Area Under the Curve ROC) is very good, we should not use here the ROC curve to assess the quality of our model.\n",
    "\n",
    "Why? First let us remind the False Positive Rate formula, which corresponds to the x axis of the ROC curve: FPR (False Positive Rate) = # False Positives / # Negatives.\n",
    "\n",
    "Here the # Negatives corresponds to our number of good reviews which is very high because our dataset is imbalanced. This means that even with some False Positives, our FPR will tend to stay very low. Our model will be able to make a lot of false positives predictions and still have a low false positive rate, while increasing the true positive rate and therefore artificially increasing the AUC ROC metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce77133401f4c7862f7aa28c7f879217f382f93a"
   },
   "outputs": [],
   "source": [
    "# PR curve\n",
    "\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.utils.fixes import signature\n",
    "\n",
    "average_precision = average_precision_score(y_test, y_pred)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "\n",
    "plt.figure(1, figsize = (15, 10))\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6a6977b4e48d28e637b85fed45064edf1725bfb"
   },
   "source": [
    "A better metric in this imbalanced situation is the AUC PR (Area Under the Curve Precision Recall), or also called AP (Average Precision). \n",
    "\n",
    "We can see that the precision decreases when we increase the recall. This shows us that we have to choose a prediction thresold adapted to our needs. If our goal is to have a high recall, we should set a low prediction thresold that will allow us to detect most of the observations of the positive class, but with a low precision. On the contrary, if we want to be really confident about our predictions but don't mind about not finding all the positive observations, we should set a high thresold that will get us a high precision and a low recall.\n",
    "\n",
    "In order to know if our model performs better than another classifier, we can simply use the AP metric. To assess the quality of our model, we can compare it to a simple decision baseline. Let's take a random classifier as a baseline here that would predict half of the time 1 and half of the time 0 for the label.\n",
    "\n",
    "Such a classifier would have a precision of 4.3%, which corresponds to the proportion of positive observations. For every recall value the precision would stay the same, and this would lead us to an AP of 0.043. The AP of our model is approximately 0.35, which is more than 8 times higher than the AP of the random method. This means that our model has a good predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1459faf9b176c226ce05ad23e4bee5832dfcba77"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1869399f441d068dfdafaf1eb6b8710a0a58dd59"
   },
   "source": [
    "It is completely possible to use only raw text as input for making predictions. The most important thing is to be able to extract the relevant features from this raw source of data. This kind of data can often come as a good complementary source in data science projects in order to extract more learning features and increase the predictive power of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd65c8f4e60ac6fddc80f1b3a88476fdc28b1cee"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
